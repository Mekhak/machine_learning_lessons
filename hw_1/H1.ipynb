{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Building basic functions with numpy ##\n",
    "\n",
    "Numpy is the main package for scientific computing in Python. It is maintained by a large community (www.numpy.org). In this exercise you will learn several key numpy functions such as np.exp, np.log, and np.reshape. You will need to know how to use these functions for future assignments.\n",
    "\n",
    "### 1.1 - sigmoid function, np.exp() ###\n",
    "\n",
    "Before using np.exp(), you will use math.exp() to implement the sigmoid function. You will then see why np.exp() is preferable to math.exp().\n",
    "\n",
    "**Exercise**: Build a function that returns the sigmoid of a real number x. Use math.exp(x) for the exponential function.\n",
    "\n",
    "**Reminder**:\n",
    "$sigmoid(x) = \\frac{1}{1+e^{-x}}$ is sometimes also known as the logistic function. It is a non-linear function used not only in Machine Learning (Logistic Regression), but also in Deep Learning.\n",
    "\n",
    "<img src=\"Sigmoid.png\" style=\"width:500px;height:228px;\">\n",
    "\n",
    "To refer to a function belonging to a specific package you could call it using package_name.function(). Run the code below to see an example with math.exp()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def basic_sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute sigmoid of x.\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s = 1 / (1 + math.exp(-x))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9525741268224334"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_sigmoid(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "<table style = \"width:40%\">\n",
    "    <tr>\n",
    "    <td>** basic_sigmoid(3) **</td> \n",
    "        <td>0.9525741268224334 </td> \n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, we rarely use the \"math\" library in deep learning because the inputs of the functions are real numbers. In deep learning we mostly use matrices and vectors. This is why numpy is more useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-8ccefa5bf989>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m### One reason why we use \"numpy\" instead of \"math\" in Deep Learning ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mbasic_sigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# you will see this give an error when you run it, because x is a vector.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-3fe9ca89f706>\u001b[0m in \u001b[0;36mbasic_sigmoid\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m### START CODE HERE ### (≈ 1 line of code)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[1;31m### END CODE HERE ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: bad operand type for unary -: 'list'"
     ]
    }
   ],
   "source": [
    "### One reason why we use \"numpy\" instead of \"math\" in Deep Learning ###\n",
    "x = [1, 2, 3]\n",
    "basic_sigmoid(x) # you will see this give an error when you run it, because x is a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, if $ x = (x_1, x_2, ..., x_n)$ is a row vector then $np.exp(x)$ will apply the exponential function to every element of x. The output will thus be: $np.exp(x) = (e^{x_1}, e^{x_2}, ..., e^{x_n})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.71828183  7.3890561  20.08553692]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# example of np.exp\n",
    "x = np.array([1, 2, 3])\n",
    "print(np.exp(x)) # result is (exp(1), exp(2), exp(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, if x is a vector, then a Python operation such as $s = x + 3$ or $s = \\frac{1}{x}$ will output s as a vector of the same size as x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example of vector operation\n",
    "x = np.array([1, 2, 3])\n",
    "print (x + 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any time you need more info on a numpy function, we encourage you to look at [the official documentation](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.exp.html). \n",
    "\n",
    "You can also create a new cell in the notebook and write `np.exp?` (for example) to get quick access to the documentation.\n",
    "\n",
    "**Exercise**: Implement the sigmoid function using numpy. \n",
    "\n",
    "**Instructions**: x could now be either a real number, a vector, or a matrix. The data structures we use in numpy to represent these shapes (vectors, matrices...) are called numpy arrays. You don't need to know more for now.\n",
    "$$ \\text{For } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n",
    "    x_1  \\\\\n",
    "    x_2  \\\\\n",
    "    ...  \\\\\n",
    "    x_n  \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
    "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
    "    ...  \\\\\n",
    "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
    "\\end{pmatrix}\\tag{1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # this means you can access numpy functions by writing np.function() instead of numpy.function()\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.88079708, 0.95257413])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "<table>\n",
    "    <tr> \n",
    "        <td> **sigmoid([1,2,3])**</td> \n",
    "        <td> array([ 0.73105858,  0.88079708,  0.95257413]) </td> \n",
    "    </tr>\n",
    "</table> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Sigmoid gradient\n",
    "\n",
    "As you've seen in lecture, you will need to compute gradients to optimize loss functions using backpropagation. Let's code your first gradient function.\n",
    "\n",
    "**Exercise**: Implement the function sigmoid_grad() to compute the gradient of the sigmoid function with respect to its input x. The formula is: $$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$\n",
    "You often code this function in two steps:\n",
    "1. Set s to be the sigmoid of x. You might find your sigmoid(x) function useful.\n",
    "2. Compute $\\sigma'(x) = s(1-s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.\n",
    "    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array\n",
    "\n",
    "    Return:\n",
    "    ds -- Your computed gradient.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    ds = s * (1 - s)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "print (\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr> \n",
    "        <td> **sigmoid_derivative([1,2,3])**</td> \n",
    "        <td> [ 0.19661193  0.10499359  0.04517666] </td> \n",
    "    </tr>\n",
    "</table> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Reshaping arrays ###\n",
    "\n",
    "Two common numpy functions used in deep learning are [np.shape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html) and [np.reshape()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html). \n",
    "- X.shape is used to get the shape (dimension) of a matrix/vector X. \n",
    "- X.reshape(...) is used to reshape X into some other dimension. \n",
    "\n",
    "For example, in computer science, an image is represented by a 3D array of shape $(length, height, depth = 3)$. However, when you read an image as the input of an algorithm you convert it to a vector of shape $(length*height*3, 1)$. In other words, you \"unroll\", or reshape, the 3D array into a 1D vector.\n",
    "\n",
    "<img src=\"image2vector.png\" style=\"width:500px;height:300;\">\n",
    "\n",
    "**Exercise**: Implement `image2vector()` that takes an input of shape (length, height, 3) and returns a vector of shape (length\\*height\\*3, 1). For example, if you would like to reshape an array v of shape (a, b, c) into a vector of shape (a*b,c) you would do:\n",
    "``` python\n",
    "v = v.reshape((v.shape[0]*v.shape[1], v.shape[2])) # v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c\n",
    "```\n",
    "- Please don't hardcode the dimensions of image as a constant. Instead look up the quantities you need with `image.shape[0]`, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: image2vector\n",
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    image -- a numpy array of shape (length, height, depth)\n",
    "    \n",
    "    Returns:\n",
    "    v -- a vector of shape (length*height*depth, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    v = image.reshape((image.shape[0] * image.shape[1] * image.shape[2], 1))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image2vector(image) = [[0.67826139]\n",
      " [0.29380381]\n",
      " [0.90714982]\n",
      " [0.52835647]\n",
      " [0.4215251 ]\n",
      " [0.45017551]\n",
      " [0.92814219]\n",
      " [0.96677647]\n",
      " [0.85304703]\n",
      " [0.52351845]\n",
      " [0.19981397]\n",
      " [0.27417313]\n",
      " [0.60659855]\n",
      " [0.00533165]\n",
      " [0.10820313]\n",
      " [0.49978937]\n",
      " [0.34144279]\n",
      " [0.94630077]]\n"
     ]
    }
   ],
   "source": [
    "# This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values\n",
    "image = np.array([[[ 0.67826139,  0.29380381],\n",
    "        [ 0.90714982,  0.52835647],\n",
    "        [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "       [[ 0.92814219,  0.96677647],\n",
    "        [ 0.85304703,  0.52351845],\n",
    "        [ 0.19981397,  0.27417313]],\n",
    "\n",
    "       [[ 0.60659855,  0.00533165],\n",
    "        [ 0.10820313,  0.49978937],\n",
    "        [ 0.34144279,  0.94630077]]])\n",
    "\n",
    "print (\"image2vector(image) = \" + str(image2vector(image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "     <tr> \n",
    "       <td> **image2vector(image)** </td> \n",
    "       <td> [[ 0.67826139]\n",
    " [ 0.29380381]\n",
    " [ 0.90714982]\n",
    " [ 0.52835647]\n",
    " [ 0.4215251 ]\n",
    " [ 0.45017551]\n",
    " [ 0.92814219]\n",
    " [ 0.96677647]\n",
    " [ 0.85304703]\n",
    " [ 0.52351845]\n",
    " [ 0.19981397]\n",
    " [ 0.27417313]\n",
    " [ 0.60659855]\n",
    " [ 0.00533165]\n",
    " [ 0.10820313]\n",
    " [ 0.49978937]\n",
    " [ 0.34144279]\n",
    " [ 0.94630077]]</td> \n",
    "     </tr>\n",
    "    \n",
    "   \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - Normalizing rows\n",
    "\n",
    "Another common technique we use in Machine Learning and Deep Learning is to normalize our data. It often leads to a better performance because gradient descent converges faster after normalization. Here, by normalization we mean changing x to $ \\frac{x}{\\| x\\|} $ (dividing each row vector of x by its norm).\n",
    "\n",
    "For example, if $$x = \n",
    "\\begin{bmatrix}\n",
    "    0 & 3 & 4 \\\\\n",
    "    2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}\\tag{3}$$ then $$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    \\sqrt{56} \\\\\n",
    "\\end{bmatrix}\\tag{4} $$and        $$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\\end{bmatrix}\\tag{5}$$ Note that you can divide matrices of different sizes and it works fine: this is called broadcasting and you're going to learn about it in part 5.\n",
    "\n",
    "\n",
    "**Exercise**: Implement normalizeRows() to normalize the rows of a matrix. After applying this function to an input matrix x, each row of x should be a vector of unit length (meaning length 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeRows(x):\n",
    "    \"\"\"\n",
    "    Implement a function that normalizes each row of the matrix x (to have unit length).\n",
    "    \n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (n, m)\n",
    "    \n",
    "    Returns:\n",
    "    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n",
    "    x_norm = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "        \n",
    "    # Divide x by its norm.\n",
    "    x = x / x_norm\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizeRows(x) = [[0.         0.6        0.8       ]\n",
      " [0.13736056 0.82416338 0.54944226]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [0, 3, 4],\n",
    "    [1, 6, 4]])\n",
    "print(\"normalizeRows(x) = \" + str(normalizeRows(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:60%\">\n",
    "    <tr> \n",
    "        <td> **normalizeRows(x)** </td>\n",
    "        <td> [[ 0.          0.6         0.8]\n",
    " [ 0.13736056  0.82416338  0.54944226]]</td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Implement Logistic Regression\n",
    "\n",
    "1. Define a **log_reg function** with arguments X, y and alpha (features, target and the learning rate of the gradient descent):\n",
    "\n",
    "- this function should implement the **gradient descent** based on X and y and the logistic cost function that you will specify. \n",
    "\n",
    "*Remember that through forward and backward propagation of the computational graph, the gradient descent should update the weights of the model. You can implement this either by loops or by vectorization (during the next class we will cover the vectorized implementation of this).\n",
    "\n",
    "2. Define a **RandomGridSearch** function that takes as input arguments the estimator (an example could be log_reg()), the number of cross validation folds, and the possible values of the hyperparameters, in this case it is the learning_rate of the gradient descent in a list format (example alpha_grid = [0.0001, 0.001, 0.01, 0.1, 1, 2, 5]). \n",
    "\n",
    "- the final form could be as follows: \n",
    "    RandomGridSearch(log_reg(), X, y, alpha, cv = 3, alpha_grid = [0.0001, 0.001, 0.01, 0.1, 1, 2, 5]).\n",
    "    \n",
    "- this function should randomly select several hyperparameter values from alpha_grid and perform cross validations for each of those values. For each alpha_grid value this function should get a list of cross validation scores. \n",
    "- use **ROC AUC score** as a score metric, you can find roc_auc_score function in scikit learn's metrics module useful for this task. \n",
    "\n",
    "*Note that the number of scores will depend on the value of \"cv\" you specify in the function. At the end, this function should return the best hyperparameter value based on the best average cross validation score.\n",
    "Using this hyperparameter value you can train your final model on the whole training set by using log_reg function with X, y, and alpha = best hyperparameter from RandomGridSearch.\n",
    "\n",
    "3. Using all of your defined functions train a logistic regression model on **Santander Customer Satisfaction** problem that you can find on Kaggle. \n",
    "- You can take several features of your choice from the dataset for training the model (for example up to 5-7 features). The target is binary. \n",
    "- Be careful with the target, **if it is imbalanced** you can use oversampling, undersampling, class weighting in cost function or any other method of dealing with the imbalanced target that you may find useful (you can do your research here). \n",
    "- After training the model, report the main performance metrics of your model: **ROC AUC score, precision and recall, F1 score**, as well as the **ROC curve and confusion matrix**.\n",
    "- Write a short description about the trained model and its performance in a way you see it. Describe the features you have used, which of them are the most important in terms of predicting the target and interpret in your words the performance metrics of the model. Use 5-7 sentences in maximum.\n",
    "\n",
    "*Note: You are free to use your preferred coding design with your preferred data structures for this assignment. The most important thing is to get the correct results. We will check it in the future by using the scikit learns logistic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogisticRegression():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y, alpha, n_iter):\n",
    "        self.alpha = alpha\n",
    "        self.n_iter = n_iter\n",
    "        \n",
    "        r = np.random.RandomState(1)\n",
    "        self.w_ = r.normal(loc = 0, scale = 0.01, size = 1 + X.shape[1])\n",
    "        self.cost_ = []\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            output = self.activation(self.net_input(X))\n",
    "            error = y - output\n",
    "            \n",
    "            self.w_[1:] += self.alpha * X.T.dot(error)\n",
    "            self.w_[0] += self.alpha * error.sum()\n",
    "            \n",
    "#             self.cost_.append(self.cost(y, output))\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        return self.w_[0] + X.dot(self.w_[1:])\n",
    "    \n",
    "    def activation(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -250, 250)))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.where(self.net_input(X) > 0, 1, 0)\n",
    "\n",
    "    def cost(self, y, y_pred):\n",
    "        return -y.dot(np.log(y_pred)) - ((1 - y).dot(np.log(1 - y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "class RandomGridSearch:\n",
    "    def __init__(self, classifier, cv, alpha_grid, n_iter):\n",
    "        self.classifier = classifier\n",
    "        self.cv = cv\n",
    "        self.alpha_grid = alpha_grid\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        for alpha in self.alpha_grid:\n",
    "            skfolds = StratifiedKFold(n_splits=self.cv, random_state=1, shuffle = True)\n",
    "            \n",
    "            y_pred = np.empty(y.shape)\n",
    "            y_pred_decision = np.empty(y.shape)\n",
    "            \n",
    "            for train_index, test_index in skfolds.split(X, y):\n",
    "                X_train_fold = X[train_index]\n",
    "                y_train_fold = y[train_index]\n",
    "                \n",
    "                X_test_fold = X[test_index]\n",
    "                y_test_fold = y[test_index]\n",
    "                \n",
    "                self.classifier.fit(X = X_train_fold, \n",
    "                                    y = y_train_fold,\n",
    "                                    alpha  = alpha,\n",
    "                                    n_iter = self.n_iter)\n",
    "                \n",
    "                # the predicted classes: thresholded values\n",
    "                y_pred_fold = self.classifier.predict(X_test_fold)\n",
    "                \n",
    "                # decisions: non thresholded values\n",
    "                y_pred_fold_decision = self.classifier.activation(self.classifier.net_input(X_test_fold))\n",
    "                                \n",
    "                y_pred[test_index] = y_pred_fold\n",
    "                y_pred_decision[test_index] = y_pred_fold_decision\n",
    "            \n",
    "            # reporting the results\n",
    "            self.report_results(y, y_pred, y_pred_decision, alpha)\n",
    "            \n",
    "        \n",
    "    def report_results(self, y, y_pred, y_pred_decision, alpha):\n",
    "        print(\"Results for model trained with learning rate: \", alpha)\n",
    "\n",
    "        print(\"Confusion Matrix:\\n\", confusion_matrix(y, y_pred))\n",
    "        print(\"Precission score: \", precision_score(y, y_pred))\n",
    "        print(\"    Recall score: \", recall_score(y, y_pred))\n",
    "        print(\"        F1 score: \", f1_score(y, y_pred))\n",
    "        print(\"   ROC AUC score: \", roc_auc_score(y, y_pred))\n",
    "#         print(\"      Total Cost: \", self.classifier.cost(y, y_pred_decision))\n",
    "        \n",
    "        print(\"----------------------------------------------\\n\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing on iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [2, 3]]\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y)\n",
    "\n",
    "# get only 2 classes (0 and 1) since we are going to do binary classification\n",
    "X_train_01_subset = X_train[(y_train == 0) | (y_train == 1)]\n",
    "y_train_01_subset = y_train[(y_train == 0) | (y_train == 1)]\n",
    "\n",
    "X_test_01_subset = X_test[(y_test == 0) | (y_test == 1)]\n",
    "y_test_01_subset = y_test[(y_test == 0) | (y_test == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for model trained with learning rate:  0.0001\n",
      "Confusion Matrix:\n",
      " [[ 0 35]\n",
      " [ 0 35]]\n",
      "Precission score:  0.5\n",
      "    Recall score:  1.0\n",
      "        F1 score:  0.6666666666666666\n",
      "   ROC AUC score:  0.5\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "Results for model trained with learning rate:  0.001\n",
      "Confusion Matrix:\n",
      " [[ 0 35]\n",
      " [ 0 35]]\n",
      "Precission score:  0.5\n",
      "    Recall score:  1.0\n",
      "        F1 score:  0.6666666666666666\n",
      "   ROC AUC score:  0.5\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "Results for model trained with learning rate:  0.01\n",
      "Confusion Matrix:\n",
      " [[35  0]\n",
      " [ 0 35]]\n",
      "Precission score:  1.0\n",
      "    Recall score:  1.0\n",
      "        F1 score:  1.0\n",
      "   ROC AUC score:  1.0\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "Results for model trained with learning rate:  0.05\n",
      "Confusion Matrix:\n",
      " [[35  0]\n",
      " [ 0 35]]\n",
      "Precission score:  1.0\n",
      "    Recall score:  1.0\n",
      "        F1 score:  1.0\n",
      "   ROC AUC score:  1.0\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "Results for model trained with learning rate:  0.1\n",
      "Confusion Matrix:\n",
      " [[35  0]\n",
      " [ 0 35]]\n",
      "Precission score:  1.0\n",
      "    Recall score:  1.0\n",
      "        F1 score:  1.0\n",
      "   ROC AUC score:  1.0\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "Results for model trained with learning rate:  1\n",
      "Confusion Matrix:\n",
      " [[35  0]\n",
      " [ 0 35]]\n",
      "Precission score:  1.0\n",
      "    Recall score:  1.0\n",
      "        F1 score:  1.0\n",
      "   ROC AUC score:  1.0\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "Results for model trained with learning rate:  5\n",
      "Confusion Matrix:\n",
      " [[34  1]\n",
      " [ 0 35]]\n",
      "Precission score:  0.9722222222222222\n",
      "    Recall score:  1.0\n",
      "        F1 score:  0.9859154929577464\n",
      "   ROC AUC score:  0.9857142857142857\n",
      "----------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = MyLogisticRegression()\n",
    "grid_search = RandomGridSearch(lr, cv=3, alpha_grid = [0.0001, 0.001, 0.01, 0.05, 0.1, 1, 5], n_iter = 50)\n",
    "grid_search.fit(X_train_01_subset, y_train_01_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### starting from alpha = 0.1 the divergence process starts in gradient descent algorithm\n",
    "### so, we will pick alpha = 0.05 as the best hyperparameter and go ahead by traing on the whole dataset with this alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3df2xcd5nv8fdjSLs3pN6mtwmJkzrNH6WdUqnFPwrIK5SLtqWdZVOEkBZaF1TdUZQsKxndsr7rK3lXd/NHkJeiG2A13nZKd9u0RReRgtU1Cb1sqrYWP+yEBBqcsBUhJXUggQZc06Yl5Ll/nBl7Mp5xPJkzPjPnfF7SaGbOnJzzdRR9/M1znvkec3dERCT+WqIegIiILA0FvohIQijwRUQSQoEvIpIQCnwRkYR4e9QDWMiVV17tbW3XRj0MEZGmMTm5/9fuvqrcZw0d+G1t1/LYYxNRD0NEpGl0ddnxSp+ppCMikhAKfBGRhFDgi4gkhAJfRCQhFPgiIgmhwBcRSQgFvohIQijwRUQSQoEvIpIQCnwRkYRQ4IuIJIQCX0QkIWoOfDO7xsz2mdmkmR02s74y+2wys9+Z2cH84+9rPa+IiFQnjNUyzwH3u/sBM7sC2G9mz7j7T0r2e97dPxzC+URE5BLUPMN395PufiD/+jVgElhX63FFRCRcodbwzexa4D3A98t8/H4zO2Rm3zKzdy9wjC1mNmFmE2fOnA5zeCIiiRZa4JvZCuDrwGfcfbrk4wPABne/GfgS8I1Kx3H3B929y927Vq4se9MWERG5BKEEvpktIwj7x919d+nn7j7t7jP516PAMjO7Ooxzi4jI4oTRpWPAw8Cku3+hwj5r8vthZrfmz/ubWs8tIiKLF0aXTg9wL/BjMzuY3/a/gHYAdx8GPgZsM7NzwBvAx93dQzi3iIgsUs2B7+4vAHaRfb4MfLnWc4mIyKXTN21FRBJCgS8ikhANHfhnz0Y9AhGR+GjowJ/51e/J5WB0NOqRiIg0v4YO/PYrp8nwEFMj4wp9EZEahdGWWT9/+qd0ZjqAA+RGYJRuNm6EVCrqgYmINJ/GDvy8QuiPPjvD2MgKjm3uJp2OelQiIs2lKQIfgtDvBPbn5mb7Cn0RkcVrmsAvKC7x5J69At51A5lM1KMSEWl8DX3RtpLOTAfZvqNkN30NJnRBV0RkMZpuhj8rlYJUisxUfrY/1Q2g2b6ISAXNG/h5hRIPHCQ3cQs5uhX6IiJlNH3gQyH0obNtlG0jkKObnh61b4qIFGvKGn5F6TTZzXto++k+xnaOk8tFPSARkcYRr8AHSKcZHGolu3kPTCj0RUQK4hf4BfnZvkJfRCQQixp+Rek0Webq+qAuHhFJrvjO8AvyM/0MD2m2LyKJFu8ZfkE6TSfq4hGRZIv/DL9YaRfPjlNRj0hEZMnUHPhmdo2Z7TOzSTM7bGZ9ZfYxM/uimb1kZj8ys45az3vJCl08fUfh+HGVeEQkMcKY4Z8D7nf3FPA+4NNmdmPJPncC1+UfW4BsCOetTSo118XTf0SzfRGJvZoD391PuvuB/OvXgElgXcludwGPeuB7wJVmtrbWc9csnQ4WYbtvIpjtK/RFJMZCreGb2bXAe4Dvl3y0DvhF0fsTzP+lUDjGFjObMLOJ02fOhDm88vKLsBWXeFTmEZE4Ci3wzWwF8HXgM+4+XfpxmT/i5Y7j7g+6e5e7d61auTKs4V1cPvRn2zc12xeRmAkl8M1sGUHYP+7uu8vscgK4puj9emAqjHOHKpWaXWu/UOKZnIx6UCIi4QijS8eAh4FJd/9Chd1GgE/mu3XeB/zO3U/Weu66yc/2284cVvumiMRGGDP8HuBe4INmdjD/SJvZVjPbmt9nFPgZ8BLwEPDXIZy3vlKpoH1zuEUXdEUkFmr+pq27v0D5Gn3xPg58utZzRSXbd5RtOyG3I3ifGVgd7YBERC5Bsr5pe6nyJZ7sR74dzPb7j0Q9IhGRqinwF6vQvjncAtOvkdtxSjdPF5GmosC/BNm+o/QwxtTIuGb7IhFyX/i9XEiBfylSKXoH2udm+wp9kSW3ezc8/vhcyLsH73eXawwXQIFfs+xwC22cJLdVs32RpeIOr78Oe/fOhf7jjwfvX39dM/1KkrEefp0NDrUCsL3/JLl+yAzdEPGIROLNDO65J3i9d2/wAPjQh4LttmDfYHJphh+iwaHW2RKP+vZF6qs49AsU9gtT4IcsO9xCdtPXaDtzmNzW8aiHIxJbhTJOseKavsynwK+H/E1W2lpnyPUfUfumSMiKa/Yf+hA8+mjwXFzTr/TnFnofdwr8OhocaqVn5SQ8u0+zfZEQmcHy5RfW7O+5J3i/fHn5so66enTRtu56B9oB2N4/TW7rOJnh7ohHJBIPH/1oENqFcC+EfrmwL+7qgWC/4v8hFB8nzswb+P80XTfe6BOPPRb1MEKzvX+aqekV0HoFPffdQCoV9YhEkqO4DFQQx66eri7b7+5d5T5TSWcJFVbf7Fk5ydhOlXhElpK6ehT4kegdaA8u6G4dJ5dDN1kRWQLq6lHgR2ZwqJXs5j30nH6KsZ3jCn1JnGo6ZmrtrrnUrp640UXbKKXT9KaBHS8zthPoCy7oqrYvcbd7d3ARtVBSKQTy8uXBxdhL3beSSl09ULmrJ44U+A2gd6AddrzMsUdmgou6fd0KfYmtajpmwuyuqaarJ64U+A2i0L65q2i2r9CXOKpmHZyw18wp3T9JYQ+q4Tec3oF2ejacCG6e3n9EtX2JpWo6ZtRdEx4FfgPqHWgPbrKSb99U6EvcVNMxo+6a8Kik06hSKXpTzF7QHevqpq0N0umoByYyv3ZeTS29EOB79sAdd8zV5ffsCT6/+25oaZnbd9euoJRTbt9KM/3z5+eOUe59UoXyV2BmXzGzU2b2YoXPN5nZ78zsYP7x92GcNwl6B9rJdB0kw0NMjYxrITaJXK1r0pjB0aOwalUQ7mbB86pV8Mwz8MQTF87e9+6Fc+fm73v0aPmwHxyE++8PQh6C5/vvD7YnXVi/8/4VuOMi+zzv7rfkH/8Y0nkToTPTQWemg0zXQaZGghKPyjwShTDuNOUO118Pv/71XLg/8QScPg1XXRXM3gvH3rULXn0Vpqfntj3xRPBnr79+/vnOn4eZGRgfnwv9++8P3s/MzP0SSKrQ1tIxs2uBp939pjKfbQI+6+4fruaYcVtLJwy7drzMsTNXMjW9grbN3SrxyJILY02aSse4++4g0Iu333578Pztby/ufMUhX9DdDQ88kIyyTqOspfN+MztkZt8ys3dX2snMtpjZhJlNnD5zZgmH1xx6B9oZHGqdne2rxCNLLYyumUrHaGmZv723N3gs9nwtLUG4F0tK2F/MUv0VHAA2uPvNwJeAb1Ta0d0fdPcud+9atXLlEg2v+RSXeHI7Tin4ZcmE0TVT6Rjnz8/fvmtX8Fjs+Qoz/GLFNf0kW5LAd/dpd5/Jvx4FlpnZ1Utx7jjrzHSQ7TtKZtU3NduXRatlXZpLWZOmXJ290jHuvx++9a257bffHpR4nnwyeF3Yt7jOX3rsQjmnuxueey54Lq7pJ9mSBL6ZrTEL/gNmZrfmz/ubpTh37KVSF8z2R0dR8EtFYXTYVHOnqXLne+KJoMOm3DFefRVWr57ryLnnHnjjDfj97+e2feIT8OabQUdP6flaWmDFigtr9g88ELxfsUJlnVD68M3sSWATcLWZnQD+AVgG4O7DwMeAbWZ2DngD+Lg38p1XmlBnpgM4wOShE4wdX09uqptMJupRSSMJa12axa5Jc7HzFQK8cIy77w5e790b/FK45565Us7Zs/DZzwbh/dnPwtQU3Hpr+f767dsv3F4I/aSHPeiOV/E0Osq2kTvUxSPzLPVdn6o9X7n9b7sNfvjD5HbdVKtRunRkqaTTcxd08zdZEYGlX5em2vOV2//ee9V1Exb9lcVUZ6aD7HAL2c17YEKhL4GlXpem2vOV2/+xx9R1ExYFftyl03Ohv+OUgj/BLqXDpjRUi5crKLe9+BiFb8ru2XPh+QodNuWOUVgnp7D/bbfBww/Dd7+rrpswKPCTIB/62Zv/RbP9BKu2w6bSmjR/+Zflt3/qU/N/cVRaB6d0zZxC984zz1y4xk5vbzC2lSvVdRMG/XUlRTp94Ww/p/bNJProRy+soRdCv/RWgQutSfPmm/D978/f/vrrQQ/9xdbBKbdmTuF/HlddFXxevMbO1VcHrZiFMRdCf/v2pf27iwN16STR6Ci7Dt3E2PH1sGEDmYHVUY9IGlClNWn+6Z/gb/92/vbPfz74gtRi1sEpt2bOQtt1w5PFW6hLR4GfZJOTbNt5PXSpZ1/KO38ePvCBuffPPRfMsCttd4dPfnJu+6OPBs+l2wo3I69muyyO2jKlvFRqrsTTf4TcjlNRj0gaSKU1ac6dK7/9j39c/Do45dbMWWh7A89Lm4pm+BIozPZV4om9SnerKt5euibNAw/MvV++PKjvv/e9F25fuxYuuwzuvHPuW7JPPhkc7xOfCC7AFrpwVq0KavXFd7Hauzeo15fbfvvtwZ8vjK+aO2wljWb4cnGpFNm+o3D8uNo3Y6zSWjqDgxduNwuCd/36+d0xf/wjrFsX1OxbWoLntWuD5Q8KYV/osLnqKmhtndtW6NJ59dW5UC/uFlqxYv72tjZ4seheetWu/yNzFPgyJx/6s+2bKvHEykJ3q5qZmd81YxY0dxV3x3z+83DffXD55cHs3T14vvxy+Ku/unB9HAhC/O1vn39nq9tum7+Wzj33BJ03pRdob7opWDvnUu+wJXNU0pHyiko8PR9ZTSoV9YAkDNXcaapSd0w16+PU8+5Y6twpTyUdqV5+tt925jBjOzXbj4tq7jRVKVCrWR+nnnfHUthXT4EvlaVSDA61kh1uma3tS3Or5k5TC93QpB77VjvmBi5ONCwFvizK7AXd/iPk+o9EPZwlV8tdoqJS7Z2mSte8KbfGTjXr8VzK2j3lfoZajyFzQrkBiiRAKkW2bxKAbTuvJ9d/hMzQDREPamns3h1cICyUEQohtHz5/CUJGkW5MVe60xQE2zs7528vXWOn0no8te5bSRjHkDm6aCuXZNvW87BhA6xaHetv6ZbOMEvv2tSIteSLjfnuuy9cdKxcH37x9krnqMe+C/1MtR4jKbS0goRvcpL9Y2+Qm7gFWq+I9Wy/GbtEmnHMEg516Uj48jdPzw63wPRr5PqPMDkZ9aDqoxm7RJpxzFJ/CnypWXa4hTZOcuyRfbG8oNuMXSLNOGapv1AC38y+YmanzOzFCp+bmX3RzF4ysx+ZWUcY55XGMTjUyuBQK22cjFXoh9klUukuUZXOu9D7hfYvrEUfxpibsTtJKgtrhv+vwB0LfH4ncF3+sQXIhnReaTCDQ62zJZ449O1Xe5eoSirdPWpwcP6+lda7qbR2TOn+EKw909ZW25irHYc0vlAC392fA15dYJe7gEc98D3gSjNbG8a5pfFkh1vI3jdB25nD5LaOX/wPNLjF3iWqkoXuHjUzc+FMf6H1bsqtHVNp/6mpYA2agmrHXO04pDmE1qVjZtcCT7v7TWU+exr4nLu/kH//HeB/uvvEQsdUl07z294/zRRradt0A+l01KOJTqW7RxVWoixWbYdNvTpy1OnTnBqhS6fcP4+yv2nMbIuZTZjZxOkzZ+o8LKm3waFWMu96Hp7dF4vZ/qUqLC9crFzYQ/UdNvXqyFGnT/wsVeCfAK4per8emCq3o7s/6O5d7t61auXKJRmc1FdnpiO4oNs6k9jQr3T3qHIXbqvtsKlXR446feJnqQJ/BPhkvlvnfcDv3P3kEp1bGkRx6Oe2jse2b79U6d2jnnsueC6u6RdU2xVUr7VmtIZNPIWylo6ZPQlsAq42sxPAPwDLANx9GBgF0sBLwOvAfWGcV5rP4FArALt2vMzYTkgNd0c8ovpraQnu5FRcsy/cGnDFigvLOtWuHVOvtWa0hk08aWkFicz2/mmmplck5iYr589fGO6l74tVu3ZMvdaa0Ro2zacRLtqKzDM41Ep28x56GGNsZ/xLPKXhXinsofxMfiHV7r9Y9TquREPLI0u00ml600C+xHNsczcbNxL72b5IFDTDl4bQO9BOz4YTbDz0VCJm+yJR0AxfGkbvQHvwIj/bp69bM32REGmGLw2nMNsf2zke62WXRZaaAl8aUu9AO9nhFnpWTqrEIxISlXSkofUOtM+WeMa6umlrI9Fr8ojUQjN8aXi9A+1kN+8hw0NMjYwzOhr1iESak2b40hzSaToBOEBuBCY3Bt/Q1UVdkcVT4EtT6cx0MHn6ZY49MsPU9AqObe5WiUdkkRT40nQK7Zv7c8FsfxSFvshiKPClaXVmOiiUeHLPXpH4m6yIXIwCX5paZ6aDzp5J9o+9QW7kNc32RRagwJfml0rRmYLZ2f5UcEE3k4l0VCINR4EvsVEo8cBBchO3kKNboS9SRIGfAB+8916my9wfuHXlSv4jZvcbCEIfOttG2TYCo21afVOkQIGfANNnzjBR5v7AXXG+SXw6TWbqAKPPzjA2soKxLs32RRT4EludmY7gy1qjwWxfJR5JOgW+xF86TZZ86J/eAKtWK/glkbSWjiRDOk227yjZm/8FJsbJ5aIekMjS0wxfkiOVglRqbraPVt+UZAkl8M3sDmAn8DYg5+6fK/l8E/BN4Fh+0253/8cwzi0X17pyZdkLtK1lLuQmQr7Es3/qILmRW8gd2kBmYHXUoxKpu5oD38zeBvwzcBtwAhg3sxF3/0nJrs+7+4drPZ9UL26tl6HIr77Z2TPJtp2Qy6muL/EXRg3/VuAld/+Zu78FfBW4K4TjitRfKkV2856grr91nNyOU1GPSKRuwgj8dcAvit6fyG8r9X4zO2Rm3zKzd1c6mJltMbMJM5s4Hec+cWkc6TTZ4RayfUfh+HGFvsRWGIFvZbZ5yfsDwAZ3vxn4EvCNSgdz9wfdvcvdu1YltcYs0UilLgh9Bb/ETRiBfwK4puj9emCqeAd3n3b3mfzrUWCZmV0dwrlFwpUP/ezN/6LZvsROGF0648B1ZrYReAX4OHB38Q5mtgb4lbu7md1K8IvmNyGcO/aqWQfnv3Z3s8xL/3MFfzDjN+PjNR07SevxzLZvbsxf0N0BbTevVvumNL2aA9/dz5nZ3wB7Cdoyv+Luh81sa/7zYeBjwDYzOwe8AXzcvUwyyTzVrIOzzJ1f2vwK25oKf9XVHDuR6/GkUmT7Jtn1jROMjaxX+6Y0vVD68PNlmtGSbcNFr78MfDmMc4ksqVSK3hT0Atu2Hie3A4W+NC0trSCySLMXdLeOk+s/EvVwRKqmwBdZrFQqaN8cboHp1xT60nQU+CKXYDb0d5zSQmzSNLR4WoM79qtfseaXv5y3/U1gTVfXBdveAt7pPu+LEX8ocyEX4ORvf8u6U2XaDi+7bN4mrcczX7bvKBzbw7aRO8j99AoyQzdEPSSRBSnwG9yfmPHK5ZfP277m7NmKHTm/nJhY1LHXXnnlojtvYtd6GYZC+2Yatm0NSjw9992g2ylKw1JJRyQE2eEWelZOcuyRfartS8NS4IuEpHegncGhVto4qdCXhqTAFwnZ4FDrbBePgl8aiQJfpA6ywy1k75sIZvtb5y9rIRIFa+QVDrpuvNEnEnKxcN2f/Rm89da87a+fP8/ylvm/l18/f575l3KD7p3S7W9CxWOU285ll/HKCy9csClRa+mEbHv/NFOshXfdoJusSN11ddl+d+8q95m6dBrFW2/xSpl2yHVvvcUrP/jBvO1ruroobdb8CcHKdT8q3RfKHnvN2bO8cv3187ZrLZ1wDQ61sj/3PKM/PUlu6woyw91RD0kSSiUdkSXQmekILui2zpDbOs7kJExORj0qSRoFvsgSKoT+sUf2MbZzXKEvS0qBL7LEBodaGRxqpWfDCcZ26oKuLB0FvkhEegfaZ0s8uR2nNNuXutNF2wiU63g5e/48a86e5W2lXTNlLrZCfi2dkm0O/KHM9jcJLv6W+oPZotfH0Vo69TE41AqT+Zus7FwPfd1amkHqRoEfgbIdL/lAnXj66UUdY3lLy7zOm66zZ3kYuLkkMao5biVqvayj/E1W2PEyYzvh2OZuNm5EwS+hU0lHpEH0DrST6TrIxkNP6YKu1IVm+CINpDPTQSfMzvZV4pEwaYYv0oB6B9pnu3gKffsitQol8M3sDjM7amYvmdnflfnczOyL+c9/ZGYdYZxXJM56B9qDZZfzwa/Ql1rVXNIxs7cB/wzcBpwAxs1sxN1/UrTbncB1+cd7gWz+OZFC6Xi57LJ5nTdngQ8B60uOrU6a5tY70D5b4hnbsIG2m1eTTkc9KmlGYdTwbwVecvefAZjZV4G7CJZ2KbgLeNSDldq+Z2ZXmtladz8ZwvmbThgdL6WLm0m89Q600zs6yv6pNeRGbmGUboW+VC2MwF8H/KLo/Qnmz97L7bMOSGTgi1ySdDq4oMsBciMwSndhs8iihBH45e6QXbrm8mL2CXY02wJsAWhfU/oVIhHpzHQwefplOHSCsePrNduXRQsj8E8A1xS9Xw9MXcI+ALj7g8CDEKyHH8L4RGKnd6AdgFRubrav0JeLCaNLZxy4zsw2mtllBEuyj5TsMwJ8Mt+t8z7gd0mt34uEqTPTQabrIFMjQfvm6GjUI5JGVnPgu/s54G+AvcAk8H/d/bCZbTWzrfndRoGfAS8BDwF/Xet5RSTQmekgO9wyG/wKfakklG/auvsoQagXbxsueu3Ap8M4l4iU15npoHBBNzcVXNDVLRWlmL5pKxIjnZkOspv3kG3bDhPj5HJRj0gaidbSEYmb/NXbLKNsG4HRNq2+KQHN8EXiKp0m03UQng1up6jZvmiGLxJjs6tvjgaz/RzdqusnmAJfJAnS6dkST+6nV8C7blDwJ5BKOiJJkU6T7TtKdtPXdEE3oTTDF0mSVApSqbnZPmrfTBLN8EWSKJ0mu3kPGR4KZvs7TkU9IlkCCnyRpEqng779vqNw/Di5HLrJSswp8EWSLpUiu3kPbT/Nt29qth9bCnwRgXSawaHWudm+Qj+WFPgiMieVmgv9/iMK/phR4IvIhfKhn71vQrP9mFHgi8h8hfZNlXhiRYEvIpWVhL66eJqbAl9EFpYP/bYzh9XF0+QU+CJycamUunhiQIEvIotX2sXTfyTqEUkVFPgiUp3iLp7p1zTbbyIKfBGpXpkuHq2+2fgU+CJy6fKhn1n1zWARNpV4GlpNgW9mV5nZM2b2n/nnlRX2+7mZ/djMDprZRC3nFJEGk0oFi7ANtwQlnv4jat9sULXO8P8O+I67Xwd8J/++kv/m7re4e1eN5xSRBpUdbqGNkxx7ZJ9m+w2o1sC/C/i3/Ot/Az5S4/FEpMkNDrUyONQ6O9uXxlFr4L/T3U8C5J9XV9jPgW+b2X4z27LQAc1si5lNmNnE6TNnahyeiESluMSjTp7GcNHAN7P/Z2YvlnncVcV5ety9A7gT+LSZfaDSju7+oLt3uXvXqpVlLwmISJPIDreQvW+CtjOHNdtvABcNfHf/c3e/qczjm8CvzGwtQP657K9xd5/KP58CngJuDe9HEJGGlv+WbhsnyfUfYXQ06gElV60lnRHgU/nXnwK+WbqDmb3DzK4ovAZuB16s8bwi0mQGh1rJvOt5eHYfua3jUQ8nkWoN/M8Bt5nZfwK35d9jZm1mVvg9/k7gBTM7BPwA+Hd331PjeUWkCXVmOoLZfuuMQj8C5u5Rj6Girhtv9InHHot6GCJSB9v7p5maXgFAZrg74tHER1eX7a/U/q5v2opIJAaHWoO+fc32l4wCX0QiVVzi0U1W6kuBLyKRGxxqJbt5Dz2MMbZzXKFfJ2+PegAiIgCk0/SmgR0vM7YTjm3uZuPGYGFOCYdm+CLSUHoH2unZcIKNh57SbD9kmuGLSMPpHWgPXuRn+/R1a6YfAs3wRaRhFWb7YzvHyW3VbL9WCnwRaWi9A+1kh1tmg1+hf+lU0hGRptA70D5b4hnr6qatDdLpqEfVXDTDF5Gm0TvQTnbzHjI8xNSIZvvV0gxfRJpLOk0nMHl67oIuqH1zMTTDF5GmVLigyzeC9k0tu3xxmuGLSNMqtG+mcgfIjcAo3arrL0CBLyJNrzPTAQShn3v2Cto23aDgL0OBLyKx0JnpoLNnkv1jb5AbeU2z/TIU+CISH6kUnSmYne1PBRd0M5lIR9UwFPgiEjuFEg8cJDdxC6Ntmu2DAl9EYioIfSjM9ic3aj2ehr7FoZmdBo5HPY6QXA38OupB1Jl+xnjQz9jcNrj7qnIfNHTgx4mZTVS6z2Rc6GeMB/2M8aUvXomIJIQCX0QkIRT4S+fBqAewBPQzxoN+xphSDV9EJCE0wxcRSQgFvohIQijw68zMvmJmp8zsxajHUg9mdo2Z7TOzSTM7bGZ9UY8pbGb2J2b2AzM7lP8Z/3fUY6oXM3ubmf3QzJ6Oeiz1YGY/N7Mfm9lBM5uIejxLTTX8OjOzDwAzwKPuflPU4wmbma0F1rr7ATO7AtgPfMTdfxLx0EJjZga8w91nzGwZ8ALQ5+7fi3hooTOz/wF0Aa3u/uGoxxM2M/s50OXucf3S1YI0w68zd38OeDXqcdSLu5909wP5168Bk8C6aEcVLg/M5N8uyz9iN1Mys/XAXwC5qMci9aHAl9CY2bXAe4DvRzuS8OVLHQeBU8Az7h67nxH4P0A/cD7qgdSRA982s/1mtiXqwSw1Bb6EwsxWAF8HPuPu01GPJ2zu/kd3vwVYD9xqZrEqz5nZh4FT7r4/6rHUWY+7dwB3Ap/Ol1wTQ4EvNcvXtb8OPO7uu6MeTz25+2+BZ4E7Ih5K2HqAzfka91eBD5rZrmiHFD53n8o/nwKeAm6NdkRLS4EvNclf0HwYmHT3L0Q9nnows1VmdmX+9X8B/hw4Eu2owuXuA+6+3t2vBT4O/Ie790Y8rFCZ2TvyjQWY2TuA24FYds9VosCvMzN7EvgucL2ZnTCz/x71mELWA9xLMCM8mH/E7VYTa4F9ZvYjYJyghh/LtsWYeyfwgpkdAn4A/Lu774l4TEtKbZkiIgmhGb6ISEIo8EVEEkKBLyKSEAp8EZGEUOCLiCSEAl9EJCEU+CIiCfH/Ae8bAU0MW+wAAAABSURBVIjLW3b6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z,  alpha = 0.2, cmap = cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], \n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.8, \n",
    "                    c=colors[idx],\n",
    "                    marker=markers[idx], \n",
    "                    label=cl, \n",
    "                    edgecolor='black')\n",
    "\n",
    "lr = MyLogisticRegression()\n",
    "lr.fit(X_train_01_subset, y_train_01_subset, alpha = 0.05, n_iter = 50)\n",
    "\n",
    "plot_decision_regions(X_train_01_subset, y_train_01_subset, lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing on Santander Customer Satisfaction problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data\\\\train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'var3', 'var15', 'imp_ent_var16_ult1', 'imp_op_var39_comer_ult1',\n",
       "       'imp_op_var39_comer_ult3', 'imp_op_var40_comer_ult1',\n",
       "       'imp_op_var40_comer_ult3', 'imp_op_var40_efect_ult1',\n",
       "       'imp_op_var40_efect_ult3',\n",
       "       ...\n",
       "       'saldo_medio_var33_hace2', 'saldo_medio_var33_hace3',\n",
       "       'saldo_medio_var33_ult1', 'saldo_medio_var33_ult3',\n",
       "       'saldo_medio_var44_hace2', 'saldo_medio_var44_hace3',\n",
       "       'saldo_medio_var44_ult1', 'saldo_medio_var44_ult3', 'var38', 'TARGET'],\n",
       "      dtype='object', length=371)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train = data[data.columns[1:-2]].values\n",
    "y_train = data[\"TARGET\"].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_std = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for model trained with learning rate:  0.0001\n",
      "Confusion Matrix:\n",
      " [[72745   267]\n",
      " [ 2964    44]]\n",
      "Precission score:  0.1414790996784566\n",
      "    Recall score:  0.014627659574468085\n",
      "        F1 score:  0.026514010244049414\n",
      "   ROC AUC score:  0.5054853632337908\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "Results for model trained with learning rate:  0.001\n",
      "Confusion Matrix:\n",
      " [[70369  2643]\n",
      " [ 2494   514]]\n",
      "Precission score:  0.16281279695913842\n",
      "    Recall score:  0.17087765957446807\n",
      "        F1 score:  0.16674776966747767\n",
      "   ROC AUC score:  0.5673390653649473\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "Results for model trained with learning rate:  0.01\n",
      "Confusion Matrix:\n",
      " [[71564  1448]\n",
      " [ 2871   137]]\n",
      "Precission score:  0.08643533123028391\n",
      "    Recall score:  0.045545212765957445\n",
      "        F1 score:  0.05965599825821904\n",
      "   ROC AUC score:  0.5128564282204848\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "Results for model trained with learning rate:  0.05\n",
      "Confusion Matrix:\n",
      " [[67048  5964]\n",
      " [ 2510   498]]\n",
      "Precission score:  0.07706592386258125\n",
      "    Recall score:  0.16555851063829788\n",
      "        F1 score:  0.10517423442449843\n",
      "   ROC AUC score:  0.5419366541029105\n",
      "----------------------------------------------\n",
      "\n",
      "\n",
      "Results for model trained with learning rate:  0.1\n",
      "Confusion Matrix:\n",
      " [[70975  2037]\n",
      " [ 2573   435]]\n",
      "Precission score:  0.17597087378640777\n",
      "    Recall score:  0.14461436170212766\n",
      "        F1 score:  0.15875912408759124\n",
      "   ROC AUC score:  0.5583574191680528\n",
      "----------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr1 = MyLogisticRegression()\n",
    "grid_search = RandomGridSearch(lr1, cv=3, alpha_grid = [0.0001, 0.001, 0.01, 0.05, 0.1, 1, 5], n_iter = 100)\n",
    "grid_search.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anna\\Anaconda3\\envs\\UdemyCVCource\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Anna\\Anaconda3\\envs\\UdemyCVCource\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Anna\\Anaconda3\\envs\\UdemyCVCource\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x209b2bcdac8>]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAbiklEQVR4nO3de3SV1ZnH8e8TIFQUBSEqchG0IGDxGpTOSLVqq2iVdsZR1HphcPBGpy6nHWmr0zpMHW9MrYIXvKC0BXQsVkasjqMyah0qgSJyKYqIGBET7gJpQ8gzf+ykiTGQE3LO2ee85/dZKytn533Jed7C+nW7z76YuyMiIvmvKHYBIiKSHgp0EZGEUKCLiCSEAl1EJCEU6CIiCdE+1ht3797d+/btG+vtRUTy0oIFC9a7e0lz16IFet++fSkrK4v19iIiecnMPtjdNQ25iIgkhAJdRCQhFOgiIgmhQBcRSQgFuohIQrQY6Gb2qJlVmNmS3Vw3M7vHzFaa2WIzOz79ZYqISEtS6aE/Bpy1h+sjgP51X2OB+9teloiItFaLge7urwIb93DLSGCaB/OALmbWI10FiogkwZIlMGNGZt8jHWPoPYEPG7XL6372OWY21szKzKyssrIyDW8tIhKPO9TUhNe7dsHLL8Pbb4f2jTeCGfz+96E9axZcfHG4L1PSEejWzM+aPTXD3ae4e6m7l5aUNLtyVUQkZ7jDunVQVRXay5bB+PFQXR2+hg0LXwDr18O3vw233x7a//qvIdQ//TS0r7kGVqyAogxORUnHry4Hejdq9wLWpuH3iohkVVUVnHsuPPRQaL/4IvToEb4DzJsXAvu996C4GP7hH+CCC8K1gw+Gxx8PQQ7QsSPcdhuccUZol5TAgAGh154p6Qj02cBldbNdhgFb3P3jNPxeEZFWc4fly6GiIrRramDOHPiwbmD4nXfgiivg/fdD+/77Q8hWV4fe88aN8MIL4dqwYfD978OQIaF9wQVQWQmDBoX2lVfCP/9zw3t/7Wtw+OEZf8TdSmXa4gzg/4AjzazczMaY2dVmdnXdLc8Bq4CVwEPAtRmrVkQSzx0WLIDnn2/42WuvwV13NbQfeQR++MOG9r33hqEQCME9ZAhMmRLaf/oTfOMbMHVqaG/eHHrSf/xjaB95JFx4IWzaFHrVL70ETz0Vru2/P9xxB/TrF9r77Qfdu6f/mdOlxd0W3f2iFq47cF3aKhKRxKqtDb3gP/0J/ud/4KijQljOmRM+MHztNejVK8wGmTwZduwIvefZs+FXvwrj0PvuC4sWhdCvt2IFvPtueN2nTxgWOeec0N5nn9Dj7tMntIcODXXUD32cdlr4qveFL2T+f4dMsZDH2VdaWuraPlckua66Cr74xTBkAaF3+53vwL//exgOOfhgmDQJrrsuzAx55BH4p3+C3r3hk09g9eoQvkVFIdhrakKPudCZ2QJ3L23umpb+i0izKitDz3lj3SqUtWvhzjsb2tOnh7Hk+va//dtne7cVFWEYo96YMXDIIeH1gQfCm282fKA4ZAjcfXcIcwhhf9JJDTNCOnVSmKdCgS5SwDZvhlWrwuvnnoMRI+CXvwztRYvC2PPy5aG9cGH4ALD+w8Zu3eDooxtCd+jQ0MOurQ3tp5+GW29teK+f/xy++93wun37cL9mL6eXhlxEEsw9jFebhQ/8Jk4MU+7ur9ug4xvfCAH95puwcyf86Edhat2VV8KWLWFGyMCB0LlzCP9PP23oRUscexpyiXYEnYik38cfh7nPZ54JZ58Nf/gDnHBC+FDx3HPDB5D33BOC3gzGjWtY6dihQ5jRUe+AA0Ivul6XLuFLcpeGXETyzI4dn53Sd8st4YNFCFPqpkwJKxrdQ2/61luha9dw/W//FtasaZjhcdZZoZcuyaAeukgO2rUr9Jw7doTt2+EnP4FLLw1j1k8/HZaYb9kSPijctg22bg1/rkOH0G7XLrRLSuAHP4j2GJJl6qGL5IDly+E//zO8Xr06zOqoD+LaWviP/4D6j5xKS8MUv44dQ/vOO+EXv2j4XfVhLoVHPXSRCKqqQoCfcgocdhg89lgYv3aHvn3D0Ej9+HXnzmHRTP2S8iOPDF8iTamHLpIl77wD990XXq9bB5df3rBnyOjRMH9+w73Tp8NFjdZox9wfRPKHeugiGVZTE+ZdT5oUAnz06DDbZMYM+Ju/CfcMHBi3RkkG9dBFMmTbtjBsMmFCmON97bXwm9+EvUUARo0KW7CKpIsCXSSNKirCNMCysrB3yZe/HA4+aN8+9MLrt10VyQQNuYi0QW0tPPxw2LJ1woSwE+DcufDss2E2yj33xK5QCol66CKttGFD2JOkoiKMjy9fHpbUQwj0FSvCvHGRbFOgi6Rg7tyGk2mKi0PP+6qrwuuJE8PqzXo9mz0iXSTzFOgizVi/Pox/159cs2ZNWMDz3nthXvjq1WHFJmT20F+R1tA/RZE6b74JX/96WGrfrh0sWRJmpUCYXlhVBUccEdqHHRavTpHdUaBLwdq6FcaObTilvagonO7+yithM6tPPmnYG3y//fL7aDIpDAp0KSjV1fCP/wgffRTmg2/bFraWBTjuuDBfvH73wU6d4tUpsjc0bVESr7YW3ngDTj4ZPvggnBC/dWvYP2X69Ib7tKmV5Dv10CWR1q8PvXCAqVNh+PAQ5v37h6PWpk6NW59IJijQJTHqT97ZuTOsyKxf1POVr8DkyQ0HFI8Y0XDAg0iSKNAlESZNgsGDw5FrHTqEE+hHjAjX+vcP+6jU7x8uklQaQ5e8NWtWOLH+e9+Db34zzFCp3+zqqqvi1iYSgwJd8srSpeFDziOOCKf0LFkSZq306gXPPBO7OpG4NOQieaOqKizwefDBMKXwiSfC7BVtQSsSqIcuOe+118JeKWeeCTfdBMOGhZ8XF4dDkEUkUKBLTtu8OZxwP2hQCPRLL41dkUjuUqBLTtqwATZuDDNUFi0KKzpFZM80hi456ZprYMCAMHOla1fo3Tt2RSK5L6VAN7OzzGyFma00s/HNXD/AzP7LzN4ys6VmNjr9pUrSzZrVsK/KzTfDQw/BaafFrUkkn7QY6GbWDpgMjAAGAxeZ2eAmt10HLHP3Y4BTgYlmprkHkpL6wyFefRVGjgxTEYcMgSuv1P4qIq2RSg/9RGClu69y92pgJjCyyT0OdDYzA/YDNgI1aa1UEmnKlLDPSkVFmMGycSN86UuxqxLJT6l8KNoT+LBRuxw4qck9k4DZwFqgM3Chu9c2/UVmNhYYC9CnT5+9qVcSwj3sp1JdHeaUFxVB9+6xqxLJb6n00JvbxsibtM8EFgGHAscCk8xs/8/9Ifcp7l7q7qUlmkBcsKZOhdNPh3XrYNy4cKCEwlyk7VIJ9HKg8RyDXoSeeGOjgVkerATeBwamp0RJiurq8H2ffaBLF/iw7r/72mvyrEhapBLo84H+Ztav7oPOUYThlcbWAKcDmNnBwJHAqnQWKvnthhvCboe//S2MGgVPPglDh8auSiRZWuwbuXuNmY0DXgDaAY+6+1Izu7ru+gPABOAxM3ubMERzo7uvz2DdkicWL4ajj4af/jRsqnVS3acv6pWLpJ+5Nx0Oz47S0lIvKyuL8t6SHfPmwZe/HL6f1PRjdBHZK2a2wN1Lm7umfpKkXX0fYdgwuOgi6NEjbj0ihUJL/yWtPvoobKR1113hKLjp00EzVEWyQ4EuaXXIIeFDz3ffDUfBiUj2aMhF0mLChLAj4m23wU9+ErsakcKkHrq0mXs4Tejee8NMFhGJQ4Eue23jRpg4Ed55B265BT75RJtpicSkQJe9NncufO97Ya55hw7QuXPsikQKm8bQpdWeeAIGDw4HNn/0ERx6aOyKRATUQ5dW+vOfwz7lEyeGtsJcJHco0CUl9UMrHTvCU0/Bz38euyIRaUpDLrJH7rBlCzz7LCxcCC+/DGeeGbsqEWmOeujyGVu3hu9VVeEAimnTwla3kyaFMz9FJHcp0OUvfv1rOPBA2L4dvvCFsOKzqipcO+OMEOwikrs05CIsWwZHHBFCGxqOh5sxI25dItI66qEXuEcegaOOgpUr4YADQu98v/1iVyUie0M99ALmHoZVFi0Kx8JBmMUiIvlJgV6gbr4Z+vWD0aPDHiwikv805FIgli2DBx9saK9eDXPmRCtHRDJAgZ5g7mFlJ4QZLNddB5s2hfYDD4QFQmbx6hOR9FKgJ0j9NrYANTXhQ85774UdO+Caa+Djj6Fr13B9330V5iJJozH0BBk+POyt8uST0L49jB0L3buHDzw7dYpdnYhkmgI9z+zcCUVFYd/xl18O+5DPnh1646NGwf77N9x7113x6hSR7NOQS45zb3j94othjviiRaFdWxvGyCsqQnvcOLjssuzXKCK5QYGew7Zvh1NOaTjabeBAuP76hiX4Z5wB8+ZB//5x6xSR3KAhlxzWqRMcdFA4fLmoCHr3httvj12ViOQq9dBzzPr1YXva558Ps1Ceegp+8IPYVYlIPlCg5wB3ePfd8Lpr13D48rp1cWsSkfyjQM8BjzwCxx4b5om3awfz58MVV8SuSkTyjcbQI9q5Ezp0gG99C/7wBygpiV2RiOQz9dAjef11GDQIqquhWzeYPDksBhIR2VspBbqZnWVmK8xspZmN3809p5rZIjNbamb/m94yk8cszGL59NPYlYhIUrQY6GbWDpgMjAAGAxeZ2eAm93QB7gPOc/ejgL/LQK15r7q64RSgv/7rsECoW7e4NYlIcqTSQz8RWOnuq9y9GpgJjGxyz8XALHdfA+DuFektMxnuvhsuvhjWrg3tIg14iUgapRIpPYEPG7XL637W2ACgq5nNNbMFZtbsAnQzG2tmZWZWVllZuXcV56EdO8L366+HmTPDBloiIumWSqA3t8mqN2m3B04AzgHOBG42swGf+0PuU9y91N1LSwpkSsfjj8NXvwq/+x0UF8OFF8auSESSKpV5FeVA70btXsDaZu5Z7+7bge1m9ipwDPBOWqrMY0OHwoABcNxxsSsRkaRLpYc+H+hvZv3MrBgYBcxucs8zwHAza29mnYCTgOXpLTW/1O+SOHgw/OIX2o9cRDKvxUB39xpgHPACIaSfdPelZna1mV1dd89y4HlgMfAm8LC7L8lc2bltxw740pfghhtiVyIihSSlpSzu/hzwXJOfPdCkfSdwZ/pKy2+XXAJ9+8auQkQKidYmppF72FSrRw8YP17TEkUkuxQ5afTSS2GoZdUqhbmIZJ9iJ41KSuC886Bn01n6IiJZoCGXNDrmGJg6NXYVIlKo1ENPg48+CqtAy8tjVyIihUyBnga/+12Ya65ThkQkJgV6GlxwQThtqLQ0diUiUsgU6G30+uvhe3Fx3DpERBTobbBkCQwfHs4EFRGJTYHeBgMHwh13hDNBRURi07TFNmjfHr7//dhViIgE6qHvhc2boXfvMLtFRCRXKND3QmUlHHYYbNgQuxIRkQYactkL/fs3zG4REckV6qG30rZt8OGHLd8nIpJtCvRWeugh6NMHFiyIXYmIyGdpyKWVzjsP9tkHjj8+diUiIp+lQG+lI44IXyIiuUZDLq3wyivw9NMNB0CLiOQSBXorLF0ajpYzi12JiMjnacilFS64ADp0iF2FiEjz1ENPUW0tHHQQXHVV7EpERJqnQE/Bjh1hMdEvfxm7EhGR3VOgp2DLFhg6NCz3FxHJVRpDT0GPHjBzZuwqRET2TD30Fqxfr8OfRSQ/KNBbMGFC2Cq3oiJ2JSIie6YhlxbccguUlIQZLiIiuUw99BZ06QI33RS7ChGRlinQ9+Cmm2DatNhViIikRoG+G+7wwguwcGHsSkREUpNSoJvZWWa2wsxWmtn4Pdw31Mx2mdn56SsxDjOYPx/uuit2JSIiqWkx0M2sHTAZGAEMBi4ys8G7ue924IV0FxnDzp3he3t9bCwieSKVHvqJwEp3X+Xu1cBMYGQz930H+DWQ9xP8XnkFiovhv/87diUiIqlLJdB7Ao1P0Syv+9lfmFlP4FvAA3v6RWY21szKzKyssrKytbVmzSGHwOWXw5AhsSsREUldKoHe3O7fTY94uBu40d137ekXufsUdy9199KSkpJUa8y6QYPgscfCkn8RkXyRyghxOdC7UbsXsLbJPaXATAsnP3QHzjazGnf/TVqqzKKKClizJpwZWqQ5QCKSR1KJrPlAfzPrZ2bFwChgduMb3L2fu/d1977AU8C1+RjmADNmhJ0V338/diUiIq3TYg/d3WvMbBxh9ko74FF3X2pmV9dd3+O4eb655BLo1UsHQYtI/jGPdOJxaWmpl5WVRXlvEZF8ZWYL3L20uWsaJW7kjjvgtttiVyEisne0bKaRhQth1x7n6YiI5C4FeiMzZyrQRSR/acilTm1t+N6uXdw6RET2lgKdsLPi0UeH04lERPKVAh2orobTToMBA2JXIiKy9zSGDnTsCPfcE7sKEZG2UQ8dWLcudgUiIm1X8IG+bl3YhOvHP45diYhI2xT8kMu++8Ljj4cPRUVE8lnBB3rnznDZZbGrEBFpu4Iectm+HR59FNavj12JiEjbFXSgT58OY8bA8uWxKxERabuCDvQxY2D1ajj55NiViIi0XUGPoRcVwWGHxa5CRCQ9CraH/uyz8Pd/D5s3x65ERCQ9CjbQP/gAXn89zHIREUmCgg30666DFSu0u6KIJEdBBvqiRfDWW2AWuxIRkfQpyECfOBEefDBsmysikhQFGeinngoHHKAeuogkS0FOWxwzJnYFIiLpV3A99CVLYOfO2FWIiKRfQQX6n/8Mw4fDuHGxKxERSb+CGnJp1w6mTYNevWJXIiKSfgUV6O3bw7nnxq5CRCQzCmbIpaoKfvYzqKyMXYmISGYUTKA/8wzccAMsXhy7EhGRzCiYIZdRo2DgQDjmmNiViIhkRsEEOsCxx8auQEQkc1IacjGzs8xshZmtNLPxzVy/xMwW1329YWY51Q9+8km44grYuDF2JSIimdNioJtZO2AyMAIYDFxkZoOb3PY+cIq7Hw1MAKaku9C2WLsW5swJy/1FRJIqlR76icBKd1/l7tXATGBk4xvc/Q1331TXnAfk1Ezv668Ps1u0Va6IJFkqgd4T+LBRu7zuZ7szBvhtcxfMbKyZlZlZWWWW5g9qR0URKRSpBHpzexI2G5Nm9lVCoN/Y3HV3n+Lupe5eWlJSknqVbfCjH4Xl/rW1WXk7EZFoUpnlUg70btTuBaxtepOZHQ08DIxw9w3pKa/tDj8ctm0LB0KLiCRZKoE+H+hvZv2Aj4BRwMWNbzCzPsAs4FJ3fyftVbbBlVfGrkBEJDtaDHR3rzGzccALQDvgUXdfamZX111/APgXoBtwn4VTI2rcvTRzZafm3XehZ0/o1Cl2JSIimWce6VPD0tJSLysry+h7nH562MPljTcy+jYiIlljZgt212FO9ErR8ePDHugiIoUg0YH+ta/FrkBEJHsSO/dj9mztrCgihSWRgV5VBeefD7feGrsSEZHsSeSQyz77QHm5FhOJSGFJZKADHHRQ7ApERLIrcUMu5eUwciQsWBC7EhGR7EpcoK9dC3/8ozblEpHCk7ghlxNPhBUrYlchIpJ9ieuhi4gUqkQF+qefwqBB8PTTsSsREcm+RAX6pk0wYAAceGDsSkREsi9RY+h9+sAzz8SuQkQkjkT10LdujV2BiEg8iQn0998PQy1PPBG7EhGROBIT6B07wo03wtChsSsREYkjMWPohx4KP/1p7CpEROJJRA+9qgoWLtRmXCJS2BIR6HPnwgknwEsvxa5ERCSeRAT6iSfCtGlw8smxKxERiScRY+jdusGll8auQkQkrrzvoVdWht75J5/ErkREJK68D/Q5c+Dyy8O2uSIihSzvA/3yy2H5cjjuuNiViIjElfeBbgYDB8auQkQkvrwO9MmTQw9d889FRPI80CsqYNUqKMrrpxARSY+8jsJbbtFiIhGRenkb6MuWwa5dUFwcuxIRkdyQl4G+bVvYVfHb345diYhI7sjLlaJFRfCrX8FRR8WuREQkd6TUQzezs8xshZmtNLPxzVw3M7un7vpiMzs+/aU2KC6Gb34T+vfP5LuIiOSXFgPdzNoBk4ERwGDgIjMb3OS2EUD/uq+xwP1prvMvNmyAc87RcXMiIk2l0kM/EVjp7qvcvRqYCYxscs9IYJoH84AuZtYjzbUCsGkTzJ8Pr76aid8uIpK/UhlD7wl82KhdDpyUwj09gY8b32RmYwk9ePr06dPaWgH44hfhvffCkXMiItIglR66NfMz34t7cPcp7l7q7qUlJSWp1Nesrl2hU6e9/uMiIomUSqCXA70btXsBTfc2TOUeERHJoFQCfT7Q38z6mVkxMAqY3eSe2cBldbNdhgFb3P3jpr9IREQyp8UxdHevMbNxwAtAO+BRd19qZlfXXX8AeA44G1gJ7ABGZ65kERFpTkoLi9z9OUJoN/7ZA41eO3BdeksTEZHWyMul/yIi8nkKdBGRhFCgi4gkhAJdRCQhLHyeGeGNzSqBD/byj3cH1qexnHygZy4MeubC0JZnPszdm12ZGS3Q28LMyty9NHYd2aRnLgx65sKQqWfWkIuISEIo0EVEEiJfA31K7AIi0DMXBj1zYcjIM+flGLqIiHxevvbQRUSkCQW6iEhC5HSg59rh1NmQwjNfUvesi83sDTM7Jkad6dTSMze6b6iZ7TKz87NZXyak8sxmdqqZLTKzpWb2v9muMd1S+Ld9gJn9l5m9VffMeb1rq5k9amYVZrZkN9fTn1/unpNfhK163wMOB4qBt4DBTe45G/gt4cSkYcDvY9edhWf+K6Br3esRhfDMje57mbDr5/mx687C33MXYBnQp659UOy6s/DMPwRur3tdAmwEimPX3oZn/gpwPLBkN9fTnl+53EPPqcOps6TFZ3b3N9x9U11zHuF0qHyWyt8zwHeAXwMV2SwuQ1J55ouBWe6+BsDd8/25U3lmBzqbmQH7EQK9Jrtlpo+7v0p4ht1Je37lcqDv7uDp1t6TT1r7PGMI/w+fz1p8ZjPrCXwLeIBkSOXveQDQ1czmmtkCM7ssa9VlRirPPAkYRDi+8m3gu+5em53yokh7fqV0wEUkaTucOo+k/Dxm9lVCoJ+c0YoyL5Vnvhu40d13hc5b3kvlmdsDJwCnA/sA/2dm89z9nUwXlyGpPPOZwCLgNOAI4EUze83dt2a6uEjSnl+5HOiFeDh1Ss9jZkcDDwMj3H1DlmrLlFSeuRSYWRfm3YGzzazG3X+TnRLTLtV/2+vdfTuw3cxeBY4B8jXQU3nm0cBtHgaYV5rZ+8BA4M3slJh1ac+vXB5yKcTDqVt8ZjPrA8wCLs3j3lpjLT6zu/dz977u3hd4Crg2j8McUvu3/Qww3Mzam1kn4CRgeZbrTKdUnnkN4b9IMLODgSOBVVmtMrvSnl8520P3AjycOsVn/hegG3BfXY+1xvN4p7oUnzlRUnlmd19uZs8Di4Fa4GF3b3b6Wz5I8e95AvCYmb1NGI640d3zdltdM5sBnAp0N7Ny4MdAB8hcfmnpv4hIQuTykIuIiLSCAl1EJCEU6CIiCaFAFxFJCAW6iEhCKNBFRBJCgS4ikhD/D8Tzv5/a/MFAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "forest_clf = LogisticRegression(random_state=42)\n",
    "\n",
    "y_probas = cross_val_predict(forest_clf, X_train_std, y_train, cv=3,method=\"predict_proba\")\n",
    "y_scores = y_probas[:, 1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train,y_scores)\n",
    "\n",
    "plt.plot(fpr, tpr, \"b:\", label=\"SGD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XHpfv",
   "launcher_item_id": "Zh0CU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
